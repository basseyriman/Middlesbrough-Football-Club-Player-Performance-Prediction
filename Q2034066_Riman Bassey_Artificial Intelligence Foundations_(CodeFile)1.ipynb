{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fa1c3e",
   "metadata": {},
   "source": [
    " ### UAV SIMULATION FOR TARGET RECOGNITION\n",
    "- AUTHOR: BASSEY RIMAN\n",
    "- ID: Q2034066\n",
    "- EMAIL: Q2034066@LIVE.TEES.AC.UK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c336f",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "This project delves into the realm of Unmanned Aerial Vehicle (UAV) simulation with a primary focus on target recognition, through advanced Reinforcement Learning (RL) techniques. The UAV, autonomously guided by RL algorithms, adeptly navigates dynamic environments. Utilizing Q-learning algorithms for adaptive decision-making, the UAV proficiently maneuvers through intricate scenarios, identifying targets amidst environmental obstacles. This simulation accurately replicates real-world challenges, offering insights into RL applications for UAVs, particularly in surveillance tasks. Employing RL, the UAV simulation emerges as a potent tool for military intelligence gathering, substantially enhancing surveillance capabilities. The project's paramount significance lies in its potential to provide a strategic edge in intelligence gathering and reconnaissance operations, advancing military capabilities in a dynamically evolving technological landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e104ba1",
   "metadata": {},
   "source": [
    "### External materials incorporated into this project include:\n",
    "\n",
    "- Military Drone Image: \n",
    "A high-quality image of a military drone is incorporated to represent the agent. This addition enhances the realism of the simulation, providing a detailed portrayal of the UAV used for target recognition.https://www.pngall.com/military-drone-png/download/50108\n",
    "\n",
    "- Simulated Aerial Shot Environment (Sky):\n",
    "A simulated aerial shot environment is utilized to replicate real-world conditions. This includes a sky environment that contributes authenticity to the UAV simulation.https://www.pexels.com/video/thick-fogs-covering-the-mountain-valley-4763084/\n",
    "\n",
    "- Spaceship Image:\n",
    "An image of a spaceship is incorporated into the project to represent the target, adding diversity to the simulation scenarios and enhancing the overall dynamic nature of the UAV's encounters.https://www.pngkey.com/detail/u2e6q8t4t4r5u2a9_star-wars-rpg-nave-star-wars-star-trek/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74c19e",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 20px;\">üë®‚ÄçüíªApplication Requirements\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f6b87",
   "metadata": {},
   "source": [
    "- Let's gear up for the target recognition adventure! First, I'm ensuring I have the latest version of Pygame for the target recognition magic. The '!pip install --upgrade pygame' command takes care of this.\n",
    "- Now, I'm bringing in Imageio, a powerhouse for video and image processing. To ensure it can handle video files properly, I'm including the 'imageio[ffmpeg]' installation command, making sure FFmpeg support is on board.\n",
    "- With Pygame upgraded and Imageio ready to roll, my coding environment is prepped and optimized for the target recognition project. Time to dive into the code and start building intelligence into my agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaf6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pygame\n",
    "#!pip install imageio\n",
    "#!pip install imageio[ffmpeg]\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d97cdb",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 20px;\">üë®‚ÄçüíªImporting Necessary Libraries</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff0e7b",
   "metadata": {},
   "source": [
    "Here, I'm bringing in the essential libraries for my target recognition project. Pygame sets the visual stage,\n",
    "random adds an element of unpredictability, math and numpy handle numerical operations, and imageio is crucial\n",
    "for working with video files. Matplotlib to create visualizations that help us understand and analyze the performance of the agent during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe7e71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde016c",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªReinforcement Learning Parameters</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc084b2e",
   "metadata": {},
   "source": [
    "In this section, I'm laying the foundation for my target recognition adventure. I define the size of state\n",
    "and action spaces and initialize the learning process with a Q-table filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d5e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE_SIZE = 4\n",
    "ACTION_SPACE_SIZE = 4\n",
    "Q_TABLE = np.zeros((STATE_SPACE_SIZE, ACTION_SPACE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd093a",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªReward Constants</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c76646",
   "metadata": {},
   "source": [
    "These constants define the rules governing rewards and penalties in my target recognition environment.\n",
    "I've assigned values for actions such as successfully identifying the target, the cost of movement, and penalties\n",
    "for collisions or taking too much time in the recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfce03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_TARGET_IDENTIFIED = 200\n",
    "REWARD_MOVEMENT = 50\n",
    "PENALTY_COLLISION = -50\n",
    "PENALTY_TIME = -20\n",
    "REWARD_DISTANCE_TO_TARGET = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605128c",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªDirection Mapping Dictionary</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733a014",
   "metadata": {},
   "source": [
    "This nifty dictionary serves as my guide, translating human-readable directions ('UP', 'DOWN', 'LEFT', 'RIGHT')\n",
    "into numerical values. It acts like a compass directing the algorithm's focus during the target recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f088309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTION_MAPPING = {'UP': 0, 'DOWN': 1, 'LEFT': 2, 'RIGHT': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ca691",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªState-related Functions</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33398a",
   "metadata": {},
   "source": [
    "Now, I'm defining functions related to the current state of my recognition agent. From calculating the state based\n",
    "on positions to choosing actions and updating my Q-table during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd36f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing exploration rate\n",
    "epsilon = 0.1\n",
    "\n",
    "def get_state():\n",
    "    \"\"\"Get the current state of the agent.\"\"\"\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Calculating the distance to the target and extracts relevant information to form the current state.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    distance_to_target = math.sqrt((R_rect.x - target_rect.x) ** 2 + (R_rect.y - target_rect.y) ** 2)\n",
    "    state_info = np.array([R_rect.x % STATE_SPACE_SIZE,\n",
    "                           R_rect.y % STATE_SPACE_SIZE,\n",
    "                           distance_to_target % STATE_SPACE_SIZE])\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Scaling and converting the state information into a format suitable for the Q-table.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    state_info[:2] /= float(STATE_SPACE_SIZE)\n",
    "    state_info[2] /= float(STATE_SPACE_SIZE)\n",
    "\n",
    "    state_info = (state_info * STATE_SPACE_SIZE).astype(int)\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "# - Printing the current state for a quick check.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    print(\"State:\", state_info)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Returns the formatted state information.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    return state_info\n",
    "#\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Choosing an action based on the current state, incorporating exploration-exploitation through epsilon-greedy strategy.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def select_action(state):\n",
    "    \"\"\"Select an action based on the current state.\"\"\"\n",
    "    state_int = state.astype(int)\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Randomly exploring with a probability of epsilon or exploits by selecting the action with the highest Q-value.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    epsilon = 0.1\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(ACTION_SPACE_SIZE)\n",
    "    else:\n",
    "        \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Returning the selected action.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        return np.argmax(Q_TABLE[state_int, :])\n",
    "#\n",
    "#\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    \"\"\"Update the Q-table based on the observed transition.\"\"\"\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Updating the Q-table based on the observed transition, applying the Q-learning algorithm.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    action = action % ACTION_SPACE_SIZE\n",
    "    alpha = 0.9\n",
    "    gamma = 1\n",
    "    max_q_next = np.max(Q_TABLE[next_state, :])\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------   \n",
    "# - Balancing the old knowledge with the new information using learning rate (alpha) and discount factor (gamma).\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    Q_TABLE[state, action] = (1 - alpha) * Q_TABLE[state, action] + alpha * (reward + gamma * max_q_next)\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------  \n",
    "# - Printing the current state for reference.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    print(\"State:\", state)\n",
    "\n",
    "\n",
    "def obstacle_collision(x, y):\n",
    "    \"\"\"Check if there is a collision with obstacles.\"\"\"\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Checking if there is a collision with obstacles by examining the proximity of the agent to predefined obstacle positions.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    for position in CIRCLE_POSITIONS:\n",
    "        circle_rect = pygame.Rect(position[0] - 15, position[1] - 15, 30, 30)\n",
    "        if circle_rect.collidepoint(x, y):\n",
    "            \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# - Returning True if a collision is detected, indicating the need for a strategic move.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "            return True # Collision detected\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedf20",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªPygame Setup</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0ce06",
   "metadata": {},
   "source": [
    "Here, I'm setting up the stage for my target recognition project. Pygame is initialized, and images for my recognition\n",
    "agent and target are loaded. I'm also defining parameters for obstacles, rewards, and penalties in the recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36ee2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# I'm going for a fullscreen display mode, and I've aptly named the adventure 'Target Tracker.'\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)\n",
    "pygame.display.set_caption('Target Tracker')\n",
    "#\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The clock is ticking, and I've set the running flag to True to keep the project alive.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "clock = pygame.time.Clock()\n",
    "running = True\n",
    "#\n",
    "#\n",
    "#  AGENT AND TARGET SETUP\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The agent 'R' and the elusive target are entering the scene! I'm loading their images and adjusting their sizes\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "R_image = pygame.image.load(\"R.png\")\n",
    "original_R_rect = R_image.get_rect()\n",
    "R_image = pygame.transform.scale(R_image, (original_R_rect.width // 8, original_R_rect.height // 8))\n",
    "R_rect = R_image.get_rect()\n",
    "#\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# For the optimal recognissance experience. The target is positioned strategically to keep things interesting.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "target_image = pygame.image.load(\"target.png\")\n",
    "target_original_rect = target_image.get_rect()\n",
    "target_width = R_rect.width\n",
    "target_height = target_original_rect.height * target_width // target_original_rect.width\n",
    "target_image = pygame.transform.scale(target_image, (target_width, target_height))\n",
    "target_rect = target_image.get_rect()\n",
    "target_rect.topleft = (screen.get_width() - target_rect.width, 0)\n",
    "\n",
    "R_rect.topleft = (random.randint(0, screen.get_width() - R_rect.width), random.randint(0, screen.get_height() - R_rect.height))\n",
    "#\n",
    "#\n",
    "# MAP AND VIDEO SETUP\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# A map is ready to capture the agent's journey, \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "map_width, map_height = 200, 200\n",
    "map_surface = pygame.Surface((map_width, map_height))\n",
    "map_rect = map_surface.get_rect(topleft=(10, 40))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# ...And the video file for background processing is loaded.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "video_path = \"vid.mp4\"\n",
    "video = imageio.get_reader(video_path)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# I've even given the agent an initial move direction to spice things up!\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "move_direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "#\n",
    "#\n",
    "# ADVENTURE VARIABLES\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The adventure flags are set! Is the target identified? Is the project paused? How many iterations so far?\n",
    "# These variables add dynamic elements to the target recognition mission.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "target_identified = False\n",
    "paused = False\n",
    "total_iteration_count = 0\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Let the Target Recognition Begin!\n",
    "# With the stage set and characters in place, it's time to embark on the target recognition journey.\n",
    "# The game loop will keep running until the mission is accomplished or the agent decides to call it quits.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Defining circle positions\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "CIRCLE_POSITIONS = [(150, 350), (400, 500), (650, 300), (900, 600), (1150, 400)]\n",
    "\n",
    "reward = 0\n",
    "max_steps_per_episode = 5\n",
    "episode_count = 0\n",
    "total_time_taken = 0.0\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Initializing reward and penalty bars\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "REWARD_BAR_MAX_LENGTH = 200  # Adjusting the maximum length of the reward bar\n",
    "PENALTY_BAR_MAX_LENGTH = 200  # Adjusting the maximum length of the penalty bar\n",
    "REWARD_BAR_LENGTH = REWARD_BAR_MAX_LENGTH // 10  # Setting default half of the bar\n",
    "PENALTY_BAR_LENGTH = PENALTY_BAR_MAX_LENGTH // 10  # Setting default half of the bar\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Initializing cumulative rewards and penalties\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "cumulative_reward = 0\n",
    "cumulative_penalty = 0\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Initializing the rate at which the bars increase\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "REWARD_INCREASE_RATE = 1\n",
    "PENALTY_INCREASE_RATE = 0.1\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Initializing lists to store data for plotting\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "episode_rewards = []\n",
    "cumulative_rewards = []\n",
    "cumulative_penalties = []\n",
    "exploration_rate_over_time = []\n",
    "time_taken_per_episode = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e58e8",
   "metadata": {},
   "source": [
    "## <div style=\"background-color:#3498db; padding:40px; border-radius: 10px;\">üë®‚ÄçüíªMain Training Loop</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c4ba1",
   "metadata": {},
   "source": [
    "Welcome to the main event! In this loop, my target recognition unfolds. I handle events, initialize episodes,\n",
    "process video frames, guide the recognition agent's movements, update the Q-table, and display the recognition state.\n",
    "It's the core of my training process for target recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95313a38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'running' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m running:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Event handling and initialization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mget():\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mQUIT:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'running' is not defined"
     ]
    }
   ],
   "source": [
    "while running:\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Event handling and initialization\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                running = False  # Pressing ESC key will exit the game loop and close the window\n",
    "\n",
    "    if episode_count < max_steps_per_episode:\n",
    "        \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Episode initialization\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        episode_count += 1\n",
    "        target_identified = False\n",
    "        paused = False\n",
    "        reward = 0\n",
    "        prev_state = None\n",
    "        prev_action = None\n",
    "        R_rect.topleft = (\n",
    "            random.randint(0, screen.get_width() - R_rect.width),\n",
    "            random.randint(0, screen.get_height() - R_rect.height),)\n",
    "        \n",
    "        total_iteration_count = 0\n",
    "        start_time = pygame.time.get_ticks()\n",
    "        \n",
    "        \n",
    "        while not target_identified:\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Event handling\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    target_identified = True  \n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        running = False  # Pressing ESC key will exit the loop and close the window\n",
    "                        target_identified = True  \n",
    "\n",
    "            if not paused:\n",
    "\n",
    "\n",
    "# VIDEO FRAME PROCESSING AND AGENT MOVEMENT\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Time to process the next frame of the video and bring it to life on the screen. \n",
    "# I'm handling potential exceptions like StopIteration or IndexError gracefully\n",
    "# to make sure the video playback is smooth and uninterrupted.\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "                try:\n",
    "                    frame = video.get_next_data()\n",
    "                except (StopIteration, IndexError):\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# If the video is at its end, let's close it and reset for a fresh start.\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "                    video.close()\n",
    "                    video = imageio.get_reader(video_path)\n",
    "        \n",
    "# -------------------------------------------------------------------------------------------------------------                   frame = video.get_next_data()\n",
    "# Converting the video frame to a Pygame-friendly format and scaling it to fit the screen.\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "                pygame_frame = pygame.image.fromstring(frame.tobytes(), frame.shape[1::-1], \"RGB\")\n",
    "                pygame_frame = pygame.transform.scale(pygame_frame, (screen.get_width(), screen.get_height()))\n",
    "        \n",
    "# -------------------------------------------------------------------------------------------------------------       \n",
    "# Placing the frame on the screen, creating the illusion of continuous motion.\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "                screen.blit(pygame_frame, (0, 0))\n",
    "    \n",
    "# -------------------------------------------------------------------------------------------------------------   \n",
    "# Time for the target recognition agent to shine! Let's determine the state, select an action, and move accordingly.\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "                if not target_identified:\n",
    "                    state = get_state()\n",
    "                    action = select_action(state)\n",
    "                \n",
    "# -------------------------------------------------------------------------------------------------------------               \n",
    "# Calculating the new position based on the selected action.\n",
    "# ------------------------------------------------------------------------------------------------------------- \n",
    "                    speed = 5\n",
    "                    angle = action * 90\n",
    "\n",
    "                    new_x = R_rect.x + speed * math.cos(math.radians(angle))\n",
    "                    new_y = R_rect.y + speed * math.sin(math.radians(angle))\n",
    "                \n",
    "# -------------------------------------------------------------------------------------------------------------                   \n",
    "# Ensuring the agent stays within the screen boundaries and avoiding collisions with obstacles.\n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "                    \n",
    "                    new_x = max(0, min(new_x, screen.get_width() - R_rect.width))\n",
    "                    new_y = max(0, min(new_y, screen.get_height() - R_rect.height))\n",
    "\n",
    "                    if not obstacle_collision(new_x, new_y):\n",
    "                        R_rect.x, R_rect.y = new_x, new_y\n",
    "# -------------------------------------------------------------------------------------------------------------                           R_rect.x, R_rect.y = new_x, new_y\n",
    "# Updating the reward based on the agent's movement.\n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "                        reward += REWARD_MOVEMENT\n",
    "    \n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "# If there was a previous state and action, let's update the Q-table to improve decision-making.\n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "                    if prev_state is not None and prev_action is not None:\n",
    "                        update_q_table(prev_state, prev_action, reward, state)\n",
    "\n",
    "                    prev_state = state\n",
    "                    prev_action = action\n",
    "                    \n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "# Placing the agent on the screen, ready for the next iteration.\n",
    "# -------------------------------------------------------------------------------------------------------------    \n",
    "                screen.blit(R_image, R_rect)\n",
    "#\n",
    "#\n",
    "# DRAWING OBSTACLES AND CONNECTIONS\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# As I embark on creating the visual environment for my target recognition quest, it's time to bring the obstacles to life.\n",
    "# Using a loop to iterate through the obstacle positions, I draw circles of blue hue representing obstacles on the screen.\n",
    "# Each obstacle is labeled with a unique identifier, adding a touch of clarity to the recognissance system.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                # Drawing 5 circles\n",
    "            for i, position in enumerate(CIRCLE_POSITIONS, start=1):\n",
    "                pygame.draw.circle(screen, (0, 0, 255), position, 15)\n",
    "                # Displaying circle number\n",
    "                font = pygame.font.Font(None, 36)\n",
    "                text = font.render(f'Obstacle {i}', True, (255, 255, 255))\n",
    "                screen.blit(text, (position[0] - 30, position[1] + 20))\n",
    "                \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Keeping an eye on potential collisions, I check if the agent intersects with any obstacle.\n",
    "# If so, a vivid green outline signals the collision, adding an extra layer of feedback.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                # Checking if Agent R encounters a circle\n",
    "                if R_rect.colliderect(pygame.Rect(position[0] - 15, position[1] - 15, 30, 30)):\n",
    "            \n",
    "                    # Drawing a green line around the circle\n",
    "                    pygame.draw.circle(screen, (0, 255, 0), position, 15, width=2)\n",
    "                    reward += PENALTY_COLLISION  # Negative reward for colliding with a circle\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Increasing the red penalty bar and decreasing the green reward bar by 10\n",
    "# ---------------------------------------------------------------------------------------------------     \n",
    "                    # Updating the penalty bar\n",
    "                    PENALTY_BAR_LENGTH = min(PENALTY_BAR_MAX_LENGTH, PENALTY_BAR_LENGTH + PENALTY_INCREASE_RATE)\n",
    "                    REWARD_BAR_LENGTH = max(1, REWARD_BAR_LENGTH - PENALTY_INCREASE_RATE)\n",
    "\n",
    "                    # Adding a penalty for collision\n",
    "                    reward += PENALTY_COLLISION\n",
    "                    \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Now, let's establish connections between certain obstacles. It's not just a random world; there's a network to navigate!\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                for connection in [(1, 3), (3, 4), (4, 2), (2, 5)]:\n",
    "                    start_circle = CIRCLE_POSITIONS[connection[0] - 1]\n",
    "                    end_circle = CIRCLE_POSITIONS[connection[1] - 1]\n",
    "                    pygame.draw.line(screen, (0, 255, 0), start_circle, end_circle, width=2)\n",
    "                    \n",
    "                screen.blit(target_image, target_rect)\n",
    "\n",
    "                \n",
    "                if R_rect.colliderect(target_rect) and not target_identified:\n",
    "                    target_identified = True\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Increasing the green reward bar and decreasing the red penalty bar by 10\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                    REWARD_BAR_LENGTH = min(REWARD_BAR_MAX_LENGTH, REWARD_BAR_LENGTH + REWARD_INCREASE_RATE)\n",
    "                    PENALTY_BAR_LENGTH = max(0, PENALTY_BAR_LENGTH - REWARD_INCREASE_RATE)\n",
    "                    reward += REWARD_TARGET_IDENTIFIED\n",
    "                    pygame.time.delay(2000)\n",
    "                    paused = True\n",
    "\n",
    "                if target_identified:\n",
    "                    \n",
    "# DISPLAYING TARGET IDENTIFIATION              \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The moment of triumph! Once the target is identified, it's time to showcase it on the screen.\n",
    "# I'm using a loop to draw lines around the target, indicating its recognized status. The red lines\n",
    "# emanate from the target's center, creating a visually striking effect.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                    for angle in range(0, 360, 5):\n",
    "                        x = int(target_rect.centerx + 25 * math.cos(math.radians(angle)))\n",
    "                        y = int(target_rect.centery + 25 * math.sin(math.radians(angle)))\n",
    "                        pygame.draw.line(screen, (255, 0, 0), target_rect.center, (x, y), 2)\n",
    "                    \n",
    "# ---------------------------------------------------------------------------------------------------                  \n",
    "# Additionally, I'm adding text to convey the exciting news to the pilot. A bold declaration\n",
    "# 'Target Identified' in a larger font is displayed at coordinates (10, 250). For added information,\n",
    "# the target's position is revealed with a smaller font at (10, 280).\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                    font = pygame.font.Font(None, 24)\n",
    "                    text_identified = font.render('Target Identified', True, (0, 0, 0))\n",
    "                    screen.blit(text_identified, (10, 250))\n",
    "\n",
    "                    text_description = font.render(f'Target Position: ({target_rect.x}, {target_rect.y})', True, (0, 0, 0))\n",
    "                    screen.blit(text_description, (10, 280))\n",
    "#\n",
    "#\n",
    "# UPDATING CUMULATIVE REWARD AND PENALTIES                   \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Keeping track of success and challenges! After every action, I calculate the cumulative rewards\n",
    "# and penalties. The green reward bar increases, while the red penalty bar decreases. Positive rewards\n",
    "# contribute to the agent's success, and only positive penalties are accumulated.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                cumulative_reward += reward\n",
    "                cumulative_penalty += max(0, -reward)  # Only accumulating positive penalties\n",
    "        \n",
    "# Updating exploration rate over time\n",
    "                exploration_rate_over_time.append(epsilon)\n",
    "    \n",
    "# Decaying exploration rate \n",
    "                epsilon *= 0.995\n",
    "        \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The lengths of both bars are updated and drawn on the screen to provide a visual representation\n",
    "# of the agent's overall performance.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                bar_width = 200\n",
    "                bar_height = 20\n",
    "\n",
    "                pygame.draw.rect(screen, (0, 255, 0),\n",
    "                                 ((screen.get_width() - bar_width) // 2, 10, REWARD_BAR_LENGTH, bar_height))  # Green reward bar\n",
    "                pygame.draw.rect(screen, (255, 0, 0),\n",
    "                                 ((screen.get_width() - bar_width) // 2, 40, PENALTY_BAR_LENGTH, bar_height))\n",
    "                \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Adding labels to the bars\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                font = pygame.font.Font(None, 24)\n",
    "                text_reward = font.render('Reward', True, (0, 0, 0))\n",
    "                text_penalty = font.render('Penalty', True, (0, 0, 0))\n",
    "                screen.blit(text_reward, ((screen.get_width() - bar_width) // 2, 5))\n",
    "                screen.blit(text_penalty, ((screen.get_width() - bar_width) // 2, 35))\n",
    "#\n",
    "#\n",
    "# DRAWING AGENT R'S POSITION ON THE MAP     \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Mapping the journey! To offer a bird's eye view of the agent's movements, I draw its position\n",
    "# on a mini-map. The agent's current location is marked by a red dot on the map, allowing pilots\n",
    "# to track their progress throughout the operations.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                pygame.draw.rect(map_surface, (255, 0, 0),\n",
    "                                 (R_rect.x * map_width // screen.get_width(), R_rect.y * map_height // screen.get_height(), 2, 2))\n",
    "                screen.blit(map_surface, map_rect)\n",
    "\n",
    "                total_iteration_count += 1\n",
    "#\n",
    "#\n",
    "# UPDATING ITERATION COUNT AND DISPLAYING TIME TAKEN\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# The clock is ticking! After each iteration, I update the total iteration count and display the\n",
    "# time taken to make decisions and complete actions. This real-time feedback gives players an\n",
    "# understanding of the pace and efficiency of the target recognition process.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                # Displaying time taken for the current episode\n",
    "                time_taken_for_current_episode = (pygame.time.get_ticks() - start_time) / 1000.0\n",
    "                time_taken_per_episode.append(time_taken_for_current_episode)\n",
    "                episode_rewards.append(cumulative_reward)\n",
    "                \n",
    "                # Drawing episode information\n",
    "                text_time = font.render(f'Time taken: {time_taken_for_current_episode:.2f} seconds', True, (0, 0, 0))\n",
    "                screen.blit(text_time, (10, 10))\n",
    "            \n",
    "                text_episode = font.render(f'Episode: {episode_count}/{max_steps_per_episode}', True, (0, 0, 0)) \n",
    "                text_episode_x = 10 + text_time.get_width() + 20  \n",
    "                text_episode_y = 10  \n",
    "                screen.blit(text_episode, (text_episode_x, text_episode_y))\n",
    "                \n",
    "#---------------------------------------------------------------------------------------------------    \n",
    "# Starting fresh! To ensure each frame is evaluated independently, I reset the reward to zero at\n",
    "# the end of the loop. This allows the agent to earn new rewards and face new challenges in the\n",
    "# subsequent frames of the target recognition adventure.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "                reward = 0\n",
    "\n",
    "            pygame.display.flip()\n",
    "            clock.tick(60)\n",
    "# ---------------------------------------------------------------------------------------------------         \n",
    "# Appending cumulative rewards and penalties \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "            cumulative_rewards.append(cumulative_reward)\n",
    "            cumulative_penalties.append(cumulative_penalty)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Measuring the time taken for the current episode\n",
    "    total_start_time = pygame.time.get_ticks()\n",
    "    end_time = time.time()\n",
    "    time_taken_for_current_episode = end_time - start_time\n",
    "    time_taken_per_episode.append(time_taken_for_current_episode)\n",
    "    \n",
    "# DATA VISUALIZATION \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Plotting Episode Rewards Over Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(episode_rewards) + 1), episode_rewards, marker='o', linestyle='-', color='b')\n",
    "plt.title('Episode Rewards Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting Penalties vs. Rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(cumulative_rewards) + 1), cumulative_rewards, color='g', label='Rewards')\n",
    "plt.bar(range(1, len(cumulative_penalties) + 1), cumulative_penalties, color='r', label='Penalties', bottom=cumulative_rewards)\n",
    "plt.title('Cumulative Rewards and Penalties Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Exploration Rate Over Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(exploration_rate_over_time) + 1), exploration_rate_over_time, marker='o', linestyle='-', color='orange')\n",
    "plt.title('Exploration Rate Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Exploration Rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Time Taken for Target Identification\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, episode_count + 1), time_taken_per_episode[:episode_count], color='purple')\n",
    "plt.title('Time Taken for Target Identification')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Time Taken (seconds)')\n",
    "plt.show()\n",
    "\n",
    "# POST-TRAINING DISPLAY AND CLEAN UP\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# As the recognition loop concludes, I take a moment to showcase the results. I print the total time invested,\n",
    "#the number of episodes completed, and then gracefully exit Pygame after a brief pause.\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Displaying the total time taken after all episodes\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "total_end_time = pygame.time.get_ticks()\n",
    "total_time_taken = (total_end_time - total_start_time) / 1000.0\n",
    "print(\"Total Time Taken for all episodes:\", total_time_taken)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Printing the number of episodes when the game loop ends\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "print(\"Number of episodes:\", episode_count)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Waiting for a moment before quitting\n",
    "pygame.time.wait(3000)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38b7d4",
   "metadata": {},
   "source": [
    "That's my journey‚Äîmerging Pygame for visual appeal, reinforcement learning for intelligence, and video processing\n",
    "to create a target recognition system where my agent learns to navigate obstacles and successfully identify the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7ae7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
